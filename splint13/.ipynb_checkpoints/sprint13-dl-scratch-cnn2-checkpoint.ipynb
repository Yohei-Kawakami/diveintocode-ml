{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# スクラッチによる実装\n",
    "NumPyなど最低限のライブラリのみを使いアルゴリズムを実装していきます。\n",
    "\n",
    "Sprint12では1次元畳み込み層を実装しましたが、Sprint13では画像に対して一般的に使われる2次元畳み込み層を実装します。また、プーリング層なども作成することで、CNNの基本形を完成させます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データセットの用意\n",
    "引き続きMNISTデータセットを使用します。2次元畳み込み層へは、28×28の状態で入力します。\n",
    "\n",
    "今回は白黒画像であるからチャンネルは1つしかありませんが、チャンネル方向の軸は用意しておく必要があります。\n",
    "\n",
    "(n_samples, n_channels, height, width)のNCHWまたは(n_samples, height, width, n_channels)のNHWCどちらかの形にしてください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN分類器クラスの作成\n",
    "2次元畳み込みニューラルネットワークモデルのクラスScratch2dCNNClassifierを作成してください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題1】2次元畳み込み層の作成\n",
    "Sprint12で作成した1次元畳み込み層を発展させ、2次元畳み込み層のクラスConv2dを作成してください。\n",
    "\n",
    "フォワードプロパゲーションの数式は以下のようになります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "a_{i,j,m} = \\sum_{k=0}^{K-1}\\sum_{s=0}^{F_{h}-1}\\sum_{t=0}^{F_{w}-1}x_{(i+s),(j+t),k}w_{s,t,k,m}+b_{m}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ai, j, m : 出力される配列のi行j列、mチャンネルの値\n",
    "<br>i: 配列の行方向のインデックス\n",
    "<br>j: 配列の列方向のインデックス\n",
    "<br>m: 出力チャンネルのインデックス\n",
    "<br>K: 入力チャンネル数\n",
    "<br>Fh, Fw: 高さ方向（h）と幅方向（w）のフィルタのサイズ\n",
    "<br>x(i+s),(j+t),k : 入力の配列の(i+s)行(j+t)列、kチャンネルの値\n",
    "<br>ws, t, k, m: 重みの配列のs行t列目。kチャンネルの入力に対して、mチャンネルへ出力する重み\n",
    "<br>bm: mチャンネルへの出力のバイアス項\n",
    "<br>全てスカラーです。\n",
    "<br>次に更新式です。1次元畳み込み層や全結合層と同じ形です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "w_{s,t,k,m}^{\\prime} = w_{s,t,k,m} - \\alpha \\frac{\\partial L}{\\partial w_{s,t,k,m}} \\\\\n",
    "b_{m}^{\\prime} = b_{m} - \\alpha \\frac{\\partial L}{\\partial b_{m}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "α: 学習率\n",
    "<br>∂L∂ws, t, k, m : ws, t, k,mに関する損失 \n",
    "<br>Lの勾配\n",
    "<br>\n",
    "<br>∂L∂bm : bmに関する損失 \n",
    "<br>Lの勾配\n",
    "<br>\n",
    "<br>勾配∂L∂ws, t, k, mや ∂L∂bmを求めるためのバックプロパゲーションの数式が以下である。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial L}{\\partial w_{s,t,k,m}} = \\sum_{i=0}^{N_{out,h}-1}\\sum_{j=0}^{N_{out,w}-1} \\frac{\\partial L}{\\partial a_{i,j,m}}x_{(i+s)(j+t),k}\\\\\n",
    "\\frac{\\partial L}{\\partial b_{m}} = \\sum_{i=0}^{N_{out,h}-1}\\sum_{j=0}^{N_{out,w}-1}\\frac{\\partial L}{\\partial a_{i,j,m}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "∂L∂ai : 勾配の配列のi行j列、mチャンネルの値\n",
    "<br>\n",
    "<br>Nout, h, Nout,w : 高さ方向（h）と幅方向（w）の出力のサイズ\n",
    "<br>\n",
    "<br>前の層に流す誤差の数式は以下です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\partial L}{\\partial x_{i,j,k}} = \\sum_{m=0}^{M-1}\\sum_{s=0}^{F_{h}-1}\\sum_{t=0}^{F_{w}-1} \\frac{\\partial L}{\\partial a_{(i-s),(j-t),m}}w_{s,t,k,m}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "∂L∂xi, j ,k : 前の層に流す誤差の配列のi列j行、kチャンネルの値\n",
    "<br>\n",
    "<br>M : 出力チャンネル数\n",
    "<br>\n",
    "<br>ただし、 i−s<0 または i−s > Nout, h−1 または j−t <0 または j−t> Nout, w−1 のとき ∂L∂a(i−s),(j−t), m = 0 です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題2】2次元畳み込み後の出力サイズ\n",
    "畳み込みを行うと特徴マップのサイズが変化します。どのように変化するかは以下の数式から求められます。この計算を行う関数を作成してください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "N_{h,out} =  \\frac{N_{h,in}+2P_{h}-F_{h}}{S_{h}} + 1\\\\\n",
    "N_{w,out} =  \\frac{N_{w,in}+2P_{w}-F_{w}}{S_{w}} + 1\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nout : 出力のサイズ（特徴量の数）\n",
    "<br>\n",
    "<br>Nin : 入力のサイズ（特徴量の数）\n",
    "<br>\n",
    "<br>P : ある方向へのパディングの数\n",
    "<br>\n",
    "<br>F : フィルタのサイズ\n",
    "<br>\n",
    "<br>S : ストライドのサイズ\n",
    "<br>\n",
    "<br>hが高さ方向、 \n",
    "<br>wが幅方向である"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題3】最大プーリング層の作成\n",
    "最大プーリング層のクラスMaxPool2Dを作成してください。プーリング層は数式で表さない方が分かりやすい部分もありますが、フォワードプロパゲーションの数式は以下のようになります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "a_{i,j,k} = \\max_{(p,q)\\in P_{i,j}}x_{p,q,k}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pi, j : i行j列への出力する場合の入力配列のインデックスの集合。 Sh×Sw の範囲内の行（p）と列（q）\n",
    "<br>\n",
    "<br>Sh, Sw : 高さ方向（h）と幅方向（w）のストライドのサイズ\n",
    "<br>\n",
    "<br>(p, q)∈Pi, j : Pi, jに含まれる行（p）と列（q）のインデックス\n",
    "<br>\n",
    "<br>ai, j, m : 出力される配列のi行j列、kチャンネルの値\n",
    "<br>\n",
    "<br>xp, q, k : 入力の配列のp行q列、kチャンネルの値\n",
    "<br>\n",
    "<br>ある範囲の中でチャンネル方向の軸は残したまま最大値を計算することになります。\n",
    "<br>\n",
    "<br>バックプロパゲーションのためには、フォワードプロパゲーションのときの最大値のインデックス \n",
    "<br>(p, q) を保持しておく必要があります。フォワード時に最大値を持っていた箇所にそのままの誤差を流し、そこ以外には0を入れるためです。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題5】学習・推定\n",
    "作成したConv2dを使用してMNISTの分類を学習・推定してください。\n",
    "\n",
    "この段階では精度は気にせず、動くことを確認してください。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題6】（アドバンス課題）LeNet\n",
    "CNNで画像認識を行う際は、フィルタサイズや層の数などを１から考えるのではなく、有名な構造を利用することが一般的です。\n",
    "\n",
    "現在では実用的に使われることはありませんが、歴史的に重要なのは1998年のLeNetです。この構造を再現して動かしてみましょう。\n",
    "\n",
    "Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "※上記論文から引用\n",
    "\n",
    "サブサンプリングとは現在のプーリングに相当するものです。現代風に以下のように作ってみることにします。活性化関数も当時はシグモイド関数ですが、ReLUとします。\n",
    "\n",
    "畳み込み層　出力チャンネル数6、フィルタサイズ5×5、ストライド1\n",
    "ReLU\n",
    "最大プーリング\n",
    "畳み込み層　出力チャンネル数16、フィルタサイズ5×5、ストライド1\n",
    "ReLU\n",
    "最大プーリング\n",
    "平滑化\n",
    "全結合層　出力ノード数120\n",
    "ReLU\n",
    "全結合層　出力ノード数84\n",
    "ReLU\n",
    "全結合層　出力ノード数10\n",
    "ソフトマックス関数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題7】（アドバンス課題）有名な画像認識モデルの調査\n",
    "CNNの代表的な構造としてははAlexNet(2012)、VGG16(2014)などがあります。こういったものはフレームワークで既に用意されていることも多いです。\n",
    "\n",
    "どういったものがあるか簡単に調べてまとめてください。名前だけでも見ておくと良いでしょう。\n",
    "\n",
    "参考\n",
    "\n",
    "Applications - Keras Documentation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題8】（アドバンス課題）平均プーリングの作成\n",
    "平均プーリング層のクラスAveragePool2Dを作成してください。\n",
    "\n",
    "範囲内の最大値ではなく、平均値を出力とするプーリング層です。\n",
    "\n",
    "画像認識関係では最大プーリング層が一般的で、平均プーリングはあまり使われません。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "【問題9】出力サイズとパラメータ数の計算\n",
    "CNNモデルを構築する際には、全結合層に入力する段階で特徴量がいくつになっているかを事前に計算する必要があります。\n",
    "\n",
    "また、巨大なモデルを扱うようになると、メモリや計算速度の関係でパラメータ数の計算は必須になってきます。フレームワークでは各層のパラメータ数を表示させることが可能ですが、意味を理解していなくては適切な調整が行えません。\n",
    "\n",
    "以下の3つの畳み込み層の出力サイズとパラメータ数を計算してください。パラメータ数についてはバイアス項も考えてください。\n",
    "\n",
    "1.\n",
    "\n",
    "入力サイズ : 144×144, 3チャンネル\n",
    "フィルタサイズ : 3×3, 6チャンネル\n",
    "ストライド : 1\n",
    "パディング : なし\n",
    "2.\n",
    "\n",
    "入力サイズ : 60×60, 24チャンネル\n",
    "フィルタサイズ : 3×3, 48チャンネル\n",
    "ストライド　: 1\n",
    "パディング : なし\n",
    "3.\n",
    "\n",
    "入力サイズ : 20×20, 10チャンネル\n",
    "フィルタサイズ: 3×3, 20チャンネル\n",
    "ストライド : 2\n",
    "パディング : なし\n",
    "＊最後の例は丁度良く畳み込みをすることができない場合です。フレームワークでは余ったピクセルを見ないという処理が行われることがあるので、その場合を考えて計算してください。端が欠けてしまうので、こういった設定は好ましくないという例です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【問題10】（アドバンス課題）フィルタサイズに関する調査\n",
    "畳み込み層にはフィルタサイズというハイパーパラメータがありますが、2次元畳み込み層において現在では3×3と1×1の使用が大半です。以下のそれぞれを調べたり、自分なりに考えて説明してください。\n",
    "\n",
    "7×7などの大きめのものではなく、3×3のフィルタが一般的に使われる理由\n",
    "高さや幅方向を持たない1×1のフィルタの効果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetMiniBatch:\n",
    "    \"\"\"\n",
    "    ミニバッチを取得するイテレータ\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "      学習データ\n",
    "    y : 次の形のndarray, shape (n_samples, 1)\n",
    "      正解値\n",
    "    batch_size : int\n",
    "      バッチサイズ\n",
    "    seed : int\n",
    "      NumPyの乱数のシード\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, batch_size = 10, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self.X = X[shuffle_index]\n",
    "        self.y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "\n",
    "    def __getitem__(self,item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self.X[p0:p1], self.y[p0:p1]        \n",
    "\n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self.X[p0:p1], self.y[p0:p1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FC:\n",
    "    \"\"\"\n",
    "    ノード数n_nodes1からn_nodes2への全結合層\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      前の層のノード数\n",
    "    n_nodes2 : int\n",
    "      後の層のノード数\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer, dropout_rate=0.5):\n",
    "        self.optimizer = optimizer\n",
    "        # 初期化\n",
    "        # initializerのメソッドを使い、self.Wとself.Bを初期化する\n",
    "        self.W = initializer.W(n_nodes1, n_nodes2)\n",
    "        self.B = initializer.B(n_nodes2)\n",
    "        self.dZ = 0\n",
    "        self.dA = 0\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.mask = None\n",
    "        self.input_X_forward = 0\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            出力\n",
    "        \"\"\" \n",
    "        self.input_X_forward = X\n",
    "        A = np.dot(X, self.W) + self.B\n",
    "        \n",
    "        return A\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        \n",
    "        dW = np.dot(self.input_X_forward.T, dA)\n",
    "        dZ = np.dot(dA, self.W.T)\n",
    "        self.dA = dA\n",
    "        self.dW = dW\n",
    "        self.dZ = dZ\n",
    "        # 更新\n",
    "        self = self.optimizer.update(self)\n",
    "        return dZ\n",
    "    \n",
    "    def dropout_forward(self, X, flag):\n",
    "        if flag:\n",
    "            self.mask = np.random.rand(*X.shape) > self.dropout_rate\n",
    "            return X * self.mask\n",
    "        else:\n",
    "            return X * (1.0 - self.dropout_rate)\n",
    "        \n",
    "    def dropout_backward(self, X): \n",
    "        return X * self.mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FC2:\n",
    "    \"\"\"\n",
    "    ノード数n_nodes1からn_nodes2への全結合層\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      前の層のノード数\n",
    "    n_nodes2 : int\n",
    "      後の層のノード数\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer, dropout_rate=0.5):\n",
    "        self.optimizer = optimizer\n",
    "        # 初期化\n",
    "        # initializerのメソッドを使い、self.Wとself.Bを初期化する\n",
    "        self.W = initializer.W(n_nodes1, n_nodes2)\n",
    "        self.B = initializer.B(n_nodes2)\n",
    "        self.W_feedback = 0\n",
    "        self.B_feedback = 0\n",
    "        self.dZ = 0\n",
    "        self.dA = 0\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.mask = None\n",
    "        self.input_X_forward = 0\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            出力\n",
    "        \"\"\" \n",
    "        self.input_X_forward = X\n",
    "        A = np.dot(X, self.W) + self.B\n",
    "        \n",
    "        return A\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        \n",
    "        #print(\"input_X_forward.shape:\",self.input_X_forward.shape)\n",
    "        #print(\"dA.shape:\",dA.shape)\n",
    "        #print(\"self X_forward:\",self.input_X_forward.shape)\n",
    "        #print(\"dA:\",dA.shape)\n",
    "        dW = np.dot(self.input_X_forward.T, dA)\n",
    "        #print(\"dW:\",dW.shape)\n",
    "        dZ = np.dot(dA, self.W.T)\n",
    "        self.dA = dA\n",
    "        self.dW = dW\n",
    "        self.dZ = dZ\n",
    "        \n",
    "        self.W_feedback = self.dW / self.dA.shape[0]\n",
    "        self.B_feedback = np.average(self.dA, axis=0)\n",
    "        \n",
    "        # 更新\n",
    "        #print(\"W.shape:\",self.W.shape)\n",
    "        self = self.optimizer.update(self)\n",
    "        return dZ\n",
    "    \n",
    "    def dropout_forward(self, X, flag):\n",
    "        if flag:\n",
    "            self.mask = np.random.rand(*X.shape) > self.dropout_rate\n",
    "            return X * self.mask\n",
    "        else:\n",
    "            return X * (1.0 - self.dropout_rate)\n",
    "        \n",
    "    def dropout_backward(self, X): \n",
    "        return X * self.mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleInitializer:\n",
    "    \"\"\"\n",
    "    ガウス分布によるシンプルな初期化\n",
    "    Parameters\n",
    "    ----------\n",
    "    sigma : float\n",
    "      ガウス分布の標準偏差\n",
    "    \"\"\"\n",
    "    def __init__(self, sigma = 0.01):\n",
    "        self.sigma = sigma\n",
    "        \n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
    "        return W\n",
    "    \n",
    "    def B(self, n_nodes2):\n",
    "        B = self.sigma * np.random.randn(1, n_nodes2)\n",
    "        return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    \"\"\"\n",
    "    確率的勾配降下法\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : 学習率\n",
    "    \"\"\"\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "    def update(self, layer):\n",
    "        \"\"\"\n",
    "        ある層の重みやバイアスの更新\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : 更新前の層のインスタンス\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        layer : 更新後の層のインスタンス\n",
    "        \"\"\"\n",
    "        layer.B = layer.B - self.lr * layer.B_feedback    \n",
    "        layer.W = layer.W - self.lr * layer.W_feedback\n",
    "        \n",
    "        return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    \"\"\"\n",
    "    活性化関数 : Sigmoid\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      前の層のノード数\n",
    "    n_nodes2 : int\n",
    "      後の層のノード数\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # 初期化\n",
    "        self.input_X_forward = 0\n",
    "    \n",
    "    def _func(self, X):\n",
    "        return 1 / (1 + np.exp(-1 * X))\n",
    "    \n",
    "    def _func_diff(self, X):\n",
    "        return (1 - self._func(X)) * self._func(X)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            出力\n",
    "        \"\"\" \n",
    "        self.input_X_forward = X\n",
    "        A = self._func(X)\n",
    "        return A\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        \n",
    "        grad = self._func_diff(self.input_X_forward)\n",
    "        dZ = grad * dA\n",
    "        return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh:\n",
    "    \"\"\"\n",
    "    活性化関数 : Sigmoid\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      前の層のノード数\n",
    "    n_nodes2 : int\n",
    "      後の層のノード数\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # 初期化\n",
    "        self.input_X_forward = 0\n",
    "    \n",
    "    def _func(self, X):\n",
    "        return np.tanh(X)\n",
    "    \n",
    "    def _func_diff(self, X):\n",
    "        return 1 - (self._func(X))**2\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            出力\n",
    "        \"\"\" \n",
    "        self.input_X_forward = X\n",
    "        A = self._func(X)\n",
    "        return A\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        \n",
    "        grad = self._func_diff(self.input_X_forward)\n",
    "        dZ = grad * dA\n",
    "        return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class softmax:\n",
    "    \"\"\"\n",
    "    活性化関数 : Sigmoid\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      前の層のノード数\n",
    "    n_nodes2 : int\n",
    "      後の層のノード数\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # 初期化\n",
    "        self.input_X_forward = 0\n",
    "        self.pred = 0\n",
    "    \n",
    "    def _func(self, X):\n",
    "        #X = X - np.max(X)\n",
    "        #tmp = np.exp(X)\n",
    "        #denominator = np.sum(tmp, axis=1)\n",
    "        #output = tmp / denominator[:, np.newaxis]\n",
    "        tmp = X - np.max(X)\n",
    "        output = np.exp(tmp) / np.sum(np.exp(tmp))\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def _func_diff(self, X):\n",
    "        return X\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            出力\n",
    "        \"\"\" \n",
    "        \n",
    "        self.input_X_forward = X\n",
    "        A = self._func(X)\n",
    "        self.pred = A\n",
    "        return A\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        dZ = self.pred - dA\n",
    "        \n",
    "        return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    \"\"\"\n",
    "    活性化関数 : Sigmoid\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      前の層のノード数\n",
    "    n_nodes2 : int\n",
    "      後の層のノード数\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        # 初期化\n",
    "        self.input_X_forward = 0\n",
    "    \n",
    "    def _func(self, X):\n",
    "        return np.maximum(0, X)\n",
    "    \n",
    "    def _func_diff(self, X):\n",
    "        return np.where( X > 0, 1, 0)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            出力\n",
    "        \"\"\" \n",
    "        self.input_X_forward = X\n",
    "        A = self._func(X)\n",
    "        return A\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        \n",
    "        grad = self._func_diff(self.input_X_forward)\n",
    "        dZ = grad * dA\n",
    "        return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XavierInitializer:\n",
    "    \"\"\"\n",
    "    Xavierによる初期化\n",
    "    Parameters\n",
    "    ----------\n",
    "    sigma : float\n",
    "      ガウス分布の標準偏差\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.n_prev_nodes = 1\n",
    "        pass\n",
    "        \n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        self.n_prev_nodes = n_nodes1\n",
    "        W = np.random.randn(n_nodes1, n_nodes2) / np.sqrt(n_nodes1)\n",
    "        return W\n",
    "    \n",
    "    def B(self, n_nodes2):\n",
    "        B = np.random.randn(1, n_nodes2) / np.sqrt(self.n_prev_nodes)\n",
    "        return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeInitializer:\n",
    "    \"\"\"\n",
    "    Heによる初期化\n",
    "    Parameters\n",
    "    ----------\n",
    "    sigma : float\n",
    "      ガウス分布の標準偏差\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.n_prev_nodes = 1\n",
    "        pass\n",
    "        \n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        self.n_prev_nodes = n_nodes1\n",
    "        W = np.random.randn(n_nodes1, n_nodes2) * np.sqrt(2 / n_nodes1)\n",
    "        return W\n",
    "    \n",
    "    def B(self, n_nodes2):\n",
    "        B = np.random.randn(1, n_nodes2) * np.sqrt(2 / self.n_prev_nodes)\n",
    "        return B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaGrad:\n",
    "    \"\"\"\n",
    "    確率的勾配降下法\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : 学習率\n",
    "    \"\"\"\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "        self.H_B = 1\n",
    "        self.H_W = 1\n",
    "    def update(self, layer):\n",
    "        \"\"\"\n",
    "        ある層の重みやバイアスの更新\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : 更新前の層のインスタンス\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        layer : 更新後の層のインスタンス\n",
    "        \"\"\"\n",
    "        \n",
    "        #dA, dWを更新＆保存\n",
    "        self.H_B = self.H_B + np.average(layer.dA)**2\n",
    "        self.H_W = self.H_W + np.average(layer.dW)**2\n",
    "        \n",
    "        layer.B = layer.B - self.lr * np.average(layer.dA, axis=0) / np.sqrt(self.H_B)\n",
    "        layer.W = layer.W - self.lr * layer.dW / layer.dA.shape[0] / np.sqrt(self.H_W)\n",
    "        \n",
    "        return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2d():\n",
    "    \"\"\"\n",
    "    2次元Convolution層\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_input_hight : 入力２次元データの高さ\n",
    "    n_input_width : 入力２次元データの幅\n",
    "    f_w : フィルタ\n",
    "    f_b :　バイアス\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    def __init__(self, n_input_hight, n_input_width, f_w, f_b, initializer, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        # 初期化\n",
    "        # initializerのメソッドを使い、self.Wとself.Bを初期化する\n",
    "        self.n_input_hight = n_input_hight\n",
    "        self.n_input_width = n_input_width\n",
    "        self.W = f_w    #(n_output, n_ch, f_size_h, f_size_w)\n",
    "        self.B = f_b    #(1, n_ch, n_output)\n",
    "        self.n_output = self.W.shape[0]\n",
    "        self.n_input_ch = self.W.shape[1]\n",
    "        self.f_hight = f_w.shape[2]\n",
    "        self.f_width = f_w.shape[3]\n",
    "        self.n_output_hight = self.n_input_hight - self.f_hight + 1\n",
    "        self.n_output_width = self.n_input_width - self.f_width + 1\n",
    "        self.input_X_forward = 0\n",
    "        self.output_X_forward = np.zeros((self.W.shape[0], self.n_output_hight))\n",
    "        self.W_feedback = np.zeros_like(self.W)\n",
    "        self.B_feedback = np.zeros_like(self.B)\n",
    "        self.Z_feedback = 0\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_ch, n_feature11, n_feature12)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_output, n_feature21, n_feature22)\n",
    "            出力\n",
    "        \"\"\" \n",
    "        \n",
    "        self.input_X_forward = X\n",
    "        batch_size = self.input_X_forward.shape[0]\n",
    "        A = np.zeros((batch_size, self.n_output, self.n_input_ch, self.n_output_hight, self.n_output_width))\n",
    "        B = self.B[0]\n",
    "        B = B.T\n",
    "        B = B[np.newaxis]\n",
    "        #batch方向の並列計算のためaxisを追加 (Batch, ch, hight, width) = > (batch, 1, ch, hight, width)\n",
    "        X = X[:,np.newaxis]\n",
    "        for h in range(self.n_output_hight):\n",
    "            h1 = h\n",
    "            h2 = h + self.f_hight\n",
    "            for w in range(self.n_output_width):\n",
    "                w1 = w\n",
    "                w2 = w + self.f_width\n",
    "                X_seg = X[:,:,:,h1:h2,w1:w2]\n",
    "\n",
    "                #print(\"X:{} W:{}\\n\".format(X_seg.shape, self.W.shape))\n",
    "                #アダマール積 (batch, 1, ch, filter_size) * (n_output, ch, filter_size)\n",
    "                tmp = np.sum(np.sum(X_seg * self.W, axis=4), axis=3)\n",
    "                tmp = tmp + B\n",
    "                A[:,:,:,h,w] = tmp\n",
    "\n",
    "        output = np.sum(A, axis=2)\n",
    "        return output\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_output, n_feature21, n_feature22)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_ch, n_feature11, n_feature12)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        \n",
    "        batch_size = self.input_X_forward.shape[0]\n",
    "        \n",
    "        #Wについて\n",
    "        #X.shape (batch_size, n_input * n_output, n_feature11, n_feature12)\n",
    "        X = np.tile(self.input_X_forward, (dA.shape[1] ,1 ,1))\n",
    "        #dL.shape (n_input * n_output, n_featue2 )\n",
    "        dL = np.zeros((batch_size, X.shape[1], dA.shape[2], dA.shape[3]))\n",
    "        for i in range(self.n_output):\n",
    "            o1 = i * self.n_input_ch\n",
    "            o2 = i * self.n_input_ch + self.n_input_ch\n",
    "            tmp = dA[:,i][:,np.newaxis]\n",
    "            dL[:,o1:o2] = np.tile(tmp, (self.n_input_ch,1 ,1))\n",
    "        \n",
    "        #print(\"dL:\",dL)\n",
    "        #入力の特徴量数 - 出力の特徴量数 +1\n",
    "        loop1 = self.n_input_hight - self.n_output_hight + 1\n",
    "        loop2 = self.n_input_width - self.n_output_width + 1\n",
    "        dW_tmp = np.zeros((batch_size, X.shape[1], loop1, loop2))\n",
    "        for h in range(loop1):\n",
    "            h1 = h\n",
    "            h2 = h + self.n_output_hight\n",
    "            for w in range(loop2):\n",
    "                w1 = w\n",
    "                w2 = w + self.n_output_width\n",
    "                dX_seg = X[:,:, h1:h2, w1:w2]\n",
    "                dW_tmp[:,:,h,w] = np.sum(np.sum(dL * dX_seg, axis=3), axis=2)\n",
    "        \n",
    "        #bacth方向の平均をとる\n",
    "        dW_tmp2 = np.average(dW_tmp, axis=0)     \n",
    "        #計算結果をフィルタサイズに整形\n",
    "        for i in range(self.n_output):\n",
    "            o1 = i * self.n_input_ch\n",
    "            o2 = i * self.n_input_ch + self.n_input_ch\n",
    "            self.W_feedback[i] = dW_tmp2[o1:o2]\n",
    "\n",
    "        #Bについて\n",
    "        #(batch_size, n_output, n_feature21, n_feature22)\n",
    "        dB = np.sum(np.sum(dA, axis=3), axis=2)\n",
    "        dB = np.average(dB, axis=0) #bacth方向の平均をとる\n",
    "        for i in range(self.n_input_ch):\n",
    "            self.B_feedback[:,i] = dB\n",
    "        \n",
    "        #Zについて Output数回す\n",
    "        self.Z_feedback = np.zeros_like(self.input_X_forward)\n",
    "        for i in range(self.n_output):\n",
    "            #損失(行列)の端の処理のため、列の前後に0列を追加（フィルタサイズから計算）\n",
    "            dA_tmp = dA[:,i][:,np.newaxis,:]\n",
    "            dA_padding = np.zeros([batch_size, 1, self.f_hight-1, dA_tmp.shape[3]])\n",
    "            dA_tmp = np.concatenate((dA_tmp, dA_padding), axis=2)\n",
    "            dA_tmp = np.concatenate((dA_padding, dA_tmp), axis=2) \n",
    "            \n",
    "            dA_padding = np.zeros([batch_size, 1, dA_tmp.shape[2], self.f_width-1])\n",
    "            dA_tmp = np.concatenate((dA_tmp, dA_padding), axis=3)\n",
    "            dA_tmp = np.concatenate((dA_padding, dA_tmp), axis=3) \n",
    "            dA_tmp = np.tile(dA_tmp, (self.n_input_ch ,1))\n",
    "            dZ_seg = np.zeros_like(self.Z_feedback)\n",
    "            \n",
    "            for h in range(self.n_input_hight):\n",
    "                h1 = h\n",
    "                h2 = h + self.f_hight\n",
    "                for w in range(self.n_input_width):\n",
    "                    w1 = w\n",
    "                    w2 = w + self.f_width\n",
    "                    \n",
    "                    dA_seg = dA_tmp[:,:,h1:h2, w1:w2]\n",
    "                    #並列計算工夫\n",
    "                    dA_seg = np.fliplr(np.fliplr(dA_seg).T).T\n",
    "                    #dA_seg = dA_seg[:,np.newaxis]\n",
    "                    tmp = np.sum(np.sum(dA_seg * self.W[i], axis=3), axis=2)\n",
    "                    #print(\"tmp.shape:\",tmp.shape)\n",
    "                    #print(\"dZ.shape:\",dZ_seg.shape)\n",
    "                    dZ_seg[:,:,h,w] = tmp\n",
    "                \n",
    "            self.Z_feedback += dZ_seg #出力数分足し算\n",
    "\n",
    "        # 更新\n",
    "        self = self.optimizer.update(self)\n",
    "        return self.Z_feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#初期化、更新インスタンスを作る\n",
    "optimizer = SGD(0.01)\n",
    "initializer = XavierInitializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.ones([2,2,2,2])\n",
    "b = np.ones([1,2,2])\n",
    "A = np.random.randint(0,10,(1,2,6,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cov = Conv2d(6,8,w,b,initializer,optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dA = Cov.forward(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_output_data_size(input_X, filter_W, padding_size_h, padding_size_w, stride_h, stride_w):\n",
    "    input_x_hight = input_X.shape[1]\n",
    "    input_x_width = input_X.shape[2]\n",
    "    output_ch = filter_W[0]\n",
    "    filter_w_hight = filter_W.shape[3]\n",
    "    filter_w_width = filter_W.shape[4]\n",
    "    \n",
    "    out_h = (input_x_hight + padding_size_h * 2 - filter_W_hight) / stride_h + 1\n",
    "    out_w = (input_x_width + padding_size_w * 2 - filter_W_width) / stride_w + 1\n",
    "    \n",
    "    return out_ch, out_h, out_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Max_pooling():\n",
    "    \n",
    "    def __init__(self, stride_h, stride_w):\n",
    "        self.h = stride_h\n",
    "        self.w = stride_w\n",
    "        self.max_pos = 0\n",
    "        self.backward_map = 0\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        X.shape (batch_size, ch, h, w)\n",
    "        \"\"\"\n",
    "        batch_size = X.shape[0]\n",
    "        ch_size = X.shape[1]\n",
    "        h_size = X.shape[2]\n",
    "        w_size = X.shape[3]\n",
    "        \n",
    "        output_size_h = (int)(h_size / self.h) \n",
    "        output_size_w = (int)(w_size / self.w)\n",
    "        output = np.zeros((batch_size, ch_size, output_size_h, output_size_w))\n",
    "        self.backward_map = np.zeros((batch_size, ch_size, output_size_h, output_size_w, self.h, self.w))\n",
    "        \n",
    "        #print(\"input:\\n\",X)\n",
    "        for n_h in range(output_size_h):\n",
    "            for n_w in range(output_size_w):\n",
    "                pos_h1 = n_h + n_h * (self.h - 1)\n",
    "                pos_h2 = pos_h1 + self.h\n",
    "                pos_w1 = n_w + n_w * (self.w - 1)\n",
    "                pos_w2 = pos_w1 + self.w\n",
    "\n",
    "                tmp = np.max(np.max(X[:,:, pos_h1:pos_h2, pos_w1:pos_w2], axis=3), axis=2)\n",
    "                #tmp = np.max(tmp, axis=2)\n",
    "                output[:,:, n_h, n_w] = tmp\n",
    "                tmp = tmp[:,:,np.newaxis,np.newaxis]\n",
    "                self.backward_map[:,:, n_h, n_w] = (X[:,:, pos_h1:pos_h2, pos_w1:pos_w2] == tmp)\n",
    "                \n",
    "        #print(\"T/F:\",self.backward_map)\n",
    "        self.backward_map = self.backward_map.astype(int)\n",
    "        \n",
    "        return output\n",
    "        \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        dA.shape (batch_size, ch, h, w)\n",
    "        \"\"\"        \n",
    "        batch_size = dA.shape[0]\n",
    "        ch_size = dA.shape[1]\n",
    "        h_size = dA.shape[2]\n",
    "        w_size = dA.shape[3]\n",
    "        \n",
    "        output_size_h = h_size * self.h\n",
    "        output_size_w = w_size * self.w\n",
    "        output = np.zeros((batch_size, ch_size, output_size_h, output_size_w))\n",
    "        for n_h in range(h_size):\n",
    "            for n_w in range(w_size):\n",
    "                pos_h1 = n_h + n_h * (self.h - 1)\n",
    "                pos_h2 = pos_h1 + self.h\n",
    "                pos_w1 = n_w + n_w * (self.w - 1)\n",
    "                pos_w2 = pos_w1 + self.w                    \n",
    "\n",
    "                #print(\"dA:\\n\", dA[:,:, n_h, n_w])\n",
    "                #print(\"back:\\n\", self.backward_map[:,:, n_h, n_w])                    \n",
    "                tmp = dA[:,:, n_h, n_w][:,:, np.newaxis, np.newaxis]\n",
    "                output[:,:, pos_h1:pos_h2, pos_w1:pos_w2] = tmp * self.backward_map[:,:, n_h, n_w]\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = Max_pooling(2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.random.randint(0,10,(1,3,6,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dA = p.forward(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten2():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.input_X_shape = 0\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        X.shape (batch_size, n_output, n_feature1, n_feature2)\n",
    "        \n",
    "        return (batch_size, n_output * n_feature1 * n_feature2)\n",
    "        \"\"\"\n",
    "        self.inout_X_shape = X.shape\n",
    "        #print(\"Flatten input x shape:\",X.shape)\n",
    "        output = X.reshape([self.inout_X_shape[0], self.inout_X_shape[1] * self.inout_X_shape[2] * self.inout_X_shape[3]])\n",
    "        return output\n",
    "    \n",
    "    def backward(self, X):\n",
    "        output = X.reshape(self.inout_X_shape)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_dnn_design = {\n",
    "    'learning_rate':0.001,\n",
    "    'total_layer':3,\n",
    "    'func_layer1':'tanh',\n",
    "    'func_layer2':'tanh',\n",
    "    'func_layer3':'softmax',\n",
    "    'node_layer0':786, \n",
    "    'node_layer1':400,\n",
    "    'node_layer2':200,\n",
    "    'node_layer3':10,\n",
    "    'initializer':'SimpleInitializer',\n",
    "    'initializer_sigma':0.05,\n",
    "    'optimizer':'SGD',\n",
    "}\n",
    "\n",
    "class ScratchDeepNeuralNetrowkClassifier2():\n",
    "    \"\"\"\n",
    "    ディープニューラルネットワーク分類器\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_epoch, batch_size, verbose = False):\n",
    "        self.verbose = verbose\n",
    "        self.batch_size = batch_size\n",
    "        self.n_epoch = n_epoch\n",
    "        self.loss = 0\n",
    "        self.loss_val = 0\n",
    "        self.activation_func = 0\n",
    "        self.affine_func = 0\n",
    "        self.n_layer = 0\n",
    "        self.layer_instance = [0 for _ in range(64)]\n",
    "        #self.activation_func = [0 for _ in range(self.dnn_design.get('total_layer'))]\n",
    "        #self.affine_func = [0 for _ in range(self.dnn_design.get('total_layer'))]\n",
    "        #self.n_layer = self.dnn_design.get('total_layer')\n",
    "        \n",
    "        #各インスタンスを生成\n",
    "        #initializerインスタンス\n",
    "        \n",
    "    def _crossentropy(self, y_pred, y):\n",
    "        #クロスエントロピーを計算する\n",
    "        INF_AVOIDANCE = 1e-8\n",
    "        cross_entropy = -1 * y * np.log(y_pred + INF_AVOIDANCE)\n",
    "        return np.sum(cross_entropy, axis=1)\n",
    "    \n",
    "    def add_layer(self, model):\n",
    "        self.layer_instance[self.n_layer] = model\n",
    "        self.n_layer += 1\n",
    "        return\n",
    "    \n",
    "    def delet_all_layer(self):\n",
    "        #add_layerでセットしたlayer情報を全てクリアする\n",
    "        self.layer_instance[0:self.n_layer] = 0\n",
    "        self.n_layer = 0\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        #lossの記録用の配列を用意\n",
    "        self.loss = [[0 for i in range(X.shape[0])] for j in range(self.n_epoch)]\n",
    "        self.loss_val = [[0 for i in range(X.shape[0])] for j in range(self.n_epoch)]\n",
    "        \n",
    "        i = 0\n",
    "        get_mini_batch = GetMiniBatch(x_train, y_train, self.batch_size)\n",
    "        for epoch in range(self.n_epoch):\n",
    "            #print(\"Proceeding Epoch:\", i+1)\n",
    "            loop_count = 0\n",
    "            sum_loss = 0\n",
    "            for mini_X_train, mini_y_train in get_mini_batch:\n",
    "                X = mini_X_train\n",
    "                #Forwardの計算\n",
    "                for layer in range(self.n_layer):\n",
    "                    #print(\"Cal forward layer:{}\".format(layer))\n",
    "                    #X = self.affine_func[layer].forward(X)\n",
    "                    #X = self.activation_func[layer].forward(X)\n",
    "                    X = self.layer_instance[layer].forward(X)\n",
    "                    #print(\"layer:{} X:\\n{}\".format(layer, X))\n",
    "                \n",
    "                #Loss計算\n",
    "                #print(\"X.shape:{} Y.shape:{}\".format(X.shape, mini_y_train.shape))\n",
    "                sum_loss += self._crossentropy(X, mini_y_train)\n",
    "                    \n",
    "                #Backwardの計算\n",
    "                dz = mini_y_train\n",
    "                for layer in reversed(range(0, self.n_layer)):\n",
    "                    #print(\"Cal backward layer:{}\".format(layer))\n",
    "                    #dz = self.activation_func[layer].backward(dz)\n",
    "                    #dz = self.affine_func[layer].backward(dz)\n",
    "                    dz = self.layer_instance[layer].backward(dz)\n",
    "                    #print(\"layer:{} dz:\\n{}\".format(layer, dz))\n",
    "                \n",
    "                loop_count += 1\n",
    "                \n",
    "            #Epoch毎のLoss計算結果表示\n",
    "            self.loss[i] = sum_loss / loop_count\n",
    "            if X_val is not None and y_val is not None:\n",
    "                y_val_pred = self._predict(X_val)\n",
    "                self.loss_val[i] = self._crossentropy(y_val_pred, y_val)\n",
    "                \n",
    "            if self.verbose:\n",
    "                #verboseをTrueにした際は学習過程などを出力する\n",
    "                print(\"Epoch:{} \\n Loss:\\n{} Loss(val):\\n{}\".format(i+1, self.loss[i], self.loss_val[i]))\n",
    "                \n",
    "            i +=1\n",
    "            \n",
    "        return\n",
    "    \n",
    "    def predict(self, X):\n",
    "        #Forwardの計算\n",
    "        for layer in range(self.n_layer):\n",
    "            #X = self.affine_func[layer].forward(X)\n",
    "            #X = self.activation_func[layer].forward(X)\n",
    "            X = self.layer_instance[layer].forward(X)\n",
    "        \n",
    "        max_val = np.max(X, axis=1)\n",
    "        mask = np.ones_like(X)\n",
    "        X[X == max_val[:,np.newaxis]] = 1\n",
    "        X[X != mask] = 0        \n",
    "        \n",
    "        return X\n",
    "\n",
    "    def _predict(self, X):\n",
    "        #Forwardの計算\n",
    "        for layer in range(self.n_layer):\n",
    "            #X = self.affine_func[layer].forward(X)\n",
    "            #X = self.activation_func[layer].forward(X)  \n",
    "            X = self.layer_instance[layer].forward(X)\n",
    "        \n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2d():\n",
    "    \"\"\"\n",
    "    2次元Convolution層\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_input_hight : 入力２次元データの高さ\n",
    "    n_input_width : 入力２次元データの幅\n",
    "    f_w : フィルタ\n",
    "    f_b :　バイアス\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    def __init__(self, n_input_hight, n_input_width, f_w, f_b, initializer, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        # 初期化\n",
    "        # initializerのメソッドを使い、self.Wとself.Bを初期化する\n",
    "        self.n_input_hight = n_input_hight\n",
    "        self.n_input_width = n_input_width\n",
    "        self.W = f_w    #(n_output, n_ch, f_size_h, f_size_w)\n",
    "        self.B = f_b    #(1, n_ch, n_output)\n",
    "        self.n_output = self.W.shape[0]\n",
    "        self.n_input_ch = self.W.shape[1]\n",
    "        self.f_hight = f_w.shape[2]\n",
    "        self.f_width = f_w.shape[3]\n",
    "        self.n_output_hight = self.n_input_hight - self.f_hight + 1\n",
    "        self.n_output_width = self.n_input_width - self.f_width + 1\n",
    "        self.input_X_forward = 0\n",
    "        self.output_X_forward = np.zeros((self.W.shape[0], self.n_output_hight))\n",
    "        self.W_feedback = np.zeros_like(self.W)\n",
    "        self.B_feedback = np.zeros_like(self.B)\n",
    "        self.Z_feedback = 0\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (batch_size, n_ch, n_feature11, n_feature12)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_output, n_feature21, n_feature22)\n",
    "            出力\n",
    "        \"\"\" \n",
    "        \n",
    "        self.input_X_forward = X\n",
    "        batch_size = self.input_X_forward.shape[0]\n",
    "        A = np.zeros((batch_size, self.n_output, self.n_input_ch, self.n_output_hight, self.n_output_width))\n",
    "        B = self.B[0]\n",
    "        B = B.T\n",
    "        B = B[np.newaxis]\n",
    "        #batch方向の並列計算のためaxisを追加 (Batch, ch, hight, width) = > (batch, 1, ch, hight, width)\n",
    "        X = X[:,np.newaxis]\n",
    "        for h in range(self.n_output_hight):\n",
    "            h1 = h\n",
    "            h2 = h + self.f_hight\n",
    "            for w in range(self.n_output_width):\n",
    "                w1 = w\n",
    "                w2 = w + self.f_width\n",
    "                X_seg = X[:,:,:,h1:h2,w1:w2]\n",
    "\n",
    "                #print(\"X:{} W:{}\\n\".format(X_seg.shape, self.W.shape))\n",
    "                #アダマール積 (batch, 1, ch, filter_size) * (n_output, ch, filter_size)\n",
    "                tmp = np.sum(np.sum(X_seg * self.W, axis=4), axis=3)\n",
    "                tmp = tmp + B\n",
    "                A[:,:,:,h,w] = tmp\n",
    "\n",
    "        output = np.sum(A, axis=2)\n",
    "        return output\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_output, n_feature21, n_feature22)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_ch, n_feature11, n_feature12)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        \n",
    "        batch_size = self.input_X_forward.shape[0]\n",
    "        \n",
    "        #Wについて\n",
    "        #X.shape (batch_size, n_input * n_output, n_feature11, n_feature12)\n",
    "        X = np.tile(self.input_X_forward, (dA.shape[1] ,1 ,1))\n",
    "        #dL.shape (n_input * n_output, n_featue2 )\n",
    "        dL = np.zeros((batch_size, X.shape[1], dA.shape[2], dA.shape[3]))\n",
    "        for i in range(self.n_output):\n",
    "            o1 = i * self.n_input_ch\n",
    "            o2 = i * self.n_input_ch + self.n_input_ch\n",
    "            tmp = dA[:,i][:,np.newaxis]\n",
    "            dL[:,o1:o2] = np.tile(tmp, (self.n_input_ch,1 ,1))\n",
    "        \n",
    "        #print(\"dL:\",dL)\n",
    "        #入力の特徴量数 - 出力の特徴量数 +1\n",
    "        loop1 = self.n_input_hight - self.n_output_hight + 1\n",
    "        loop2 = self.n_input_width - self.n_output_width + 1\n",
    "        dW_tmp = np.zeros((batch_size, X.shape[1], loop1, loop2))\n",
    "        for h in range(loop1):\n",
    "            h1 = h\n",
    "            h2 = h + self.n_output_hight\n",
    "            for w in range(loop2):\n",
    "                w1 = w\n",
    "                w2 = w + self.n_output_width\n",
    "                dX_seg = X[:,:, h1:h2, w1:w2]\n",
    "                dW_tmp[:,:,h,w] = np.sum(np.sum(dL * dX_seg, axis=3), axis=2)\n",
    "        \n",
    "        #bacth方向の平均をとる\n",
    "        dW_tmp2 = np.average(dW_tmp, axis=0)     \n",
    "        #計算結果をフィルタサイズに整形\n",
    "        for i in range(self.n_output):\n",
    "            o1 = i * self.n_input_ch\n",
    "            o2 = i * self.n_input_ch + self.n_input_ch\n",
    "            self.W_feedback[i] = dW_tmp2[o1:o2]\n",
    "\n",
    "        #Bについて\n",
    "        #(batch_size, n_output, n_feature21, n_feature22)\n",
    "        dB = np.sum(np.sum(dA, axis=3), axis=2)\n",
    "        dB = np.average(dB, axis=0) #bacth方向の平均をとる\n",
    "        for i in range(self.n_input_ch):\n",
    "            self.B_feedback[:,i] = dB\n",
    "        \n",
    "        #Zについて Output数回す\n",
    "        self.Z_feedback = np.zeros_like(self.input_X_forward)\n",
    "        for i in range(self.n_output):\n",
    "            #損失(行列)の端の処理のため、列の前後に0列を追加（フィルタサイズから計算）\n",
    "            dA_tmp = dA[:,i][:,np.newaxis,:]\n",
    "            dA_padding = np.zeros([batch_size, 1, self.f_hight-1, dA_tmp.shape[3]])\n",
    "            dA_tmp = np.concatenate((dA_tmp, dA_padding), axis=2)\n",
    "            dA_tmp = np.concatenate((dA_padding, dA_tmp), axis=2) \n",
    "            \n",
    "            dA_padding = np.zeros([batch_size, 1, dA_tmp.shape[2], self.f_width-1])\n",
    "            dA_tmp = np.concatenate((dA_tmp, dA_padding), axis=3)\n",
    "            dA_tmp = np.concatenate((dA_padding, dA_tmp), axis=3) \n",
    "            dA_tmp = np.tile(dA_tmp, (self.n_input_ch ,1))\n",
    "            dZ_seg = np.zeros_like(self.Z_feedback)\n",
    "            \n",
    "            for h in range(self.n_input_hight):\n",
    "                h1 = h\n",
    "                h2 = h + self.f_hight\n",
    "                for w in range(self.n_input_width):\n",
    "                    w1 = w\n",
    "                    w2 = w + self.f_width\n",
    "                    \n",
    "                    dA_seg = dA_tmp[:,:,h1:h2, w1:w2]\n",
    "                    #並列計算工夫\n",
    "                    dA_seg = np.fliplr(np.fliplr(dA_seg).T).T\n",
    "                    #dA_seg = dA_seg[:,np.newaxis]\n",
    "                    tmp = np.sum(np.sum(dA_seg * self.W[i], axis=3), axis=2)\n",
    "                    #print(\"tmp.shape:\",tmp.shape)\n",
    "                    #print(\"dZ.shape:\",dZ_seg.shape)\n",
    "                    dZ_seg[:,:,h,w] = tmp\n",
    "                \n",
    "            self.Z_feedback += dZ_seg #出力数分足し算\n",
    "\n",
    "        # 更新\n",
    "        self = self.optimizer.update(self)\n",
    "        return self.Z_feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 1, 28, 28)\n",
      "(57000, 1, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "#x_train = x_train.reshape(-1, 784)\n",
    "#x_test = x_test.reshape(-1, 784)\n",
    "\n",
    "#0~255を0~1スケールへ\n",
    "x_train = x_train.astype(np.float)\n",
    "x_test = x_test.astype(np.float)\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "#0~9をone-hot encoding\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "y_train_one_hot = enc.fit_transform(y_train[:, np.newaxis])\n",
    "y_test_one_hot = enc.transform(y_test[:, np.newaxis])\n",
    "\n",
    "#chを追加\n",
    "x_train = x_train[:,np.newaxis,:]\n",
    "x_test = x_test[:,np.newaxis,:]\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train_one_hot, test_size=0.95)\n",
    "print(x_train.shape) # (48000, 784)\n",
    "print(x_val.shape) # (12000, 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.random.randn(2,1,2,2)\n",
    "b = np.random.randn(1,1,2)\n",
    "#A = np.random.randint(0,10,(1,2,6,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN2 = ScratchDeepNeuralNetrowkClassifier2(10, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DNNデザイン\n",
    "CNN2.add_layer(Conv2d(x_train.shape[2],x_train.shape[3], w, b, initializer, optimizer))\n",
    "CNN2.add_layer(Flatten2())\n",
    "CNN2.add_layer(FC2(w.shape[0] * (x_train.shape[2] - w.shape[2] + 1) * (x_train.shape[3] - w.shape[3] + 1), 100, initializer, optimizer))\n",
    "CNN2.add_layer(Sigmoid())\n",
    "CNN2.add_layer(FC2(100, 10, initializer, optimizer))\n",
    "CNN2.add_layer(softmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN2.fit(x_train, y_train, x_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = CNN2.predict(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score=0.101\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy score={:.3f}\".format(accuracy_score(y_pred, y_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss = np.array(CNN2.loss)\n",
    "loss_ave = np.average(loss, axis=1)\n",
    "\n",
    "loss_val = np.array(CNN2.loss_val)\n",
    "loss_val_ave = np.average(loss_val, axis=1)\n",
    "\n",
    "plt.title(\"epoch vs loss\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.plot(loss_ave, \"r\", label=\"loss\")\n",
    "plt.plot(loss_val_ave, \"b\", label=\"val_loss\")\n",
    "plt.legend()\n",
    "#plt.yscale(\"Log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
